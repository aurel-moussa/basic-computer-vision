#Support Vector Machine is a ML method used for classification
#Not all datasets are linearly seperable, e.g., everything above a line belonging to class 1, everything below a line belong to class 0
#We can transform the data to make a space to make it linearly seperable (e.g., log10, or x2 transformation)
#And then use a line to seperate on the transformed dataset

#It can be quite difficult to calculate the maping, so we use a shortcut: Kernel
#Variety of kernels with different ad- and disadvantages, e.g., radiacal basis function (RBF)
#RBF you can use different values of gamma. The more gamma, the more likely you are going to be able to 
#non-linearly fit datapoints, but also the more likely you are going to overfit
#In order to prevent overfitting, we want to use a validation set again, after each training round

#SVM try to find a line that has the biggest margin between two classes; you do NOT want a line that
#is very close to the cluster of group 0, but not so close to the cluster of group 1

#So we look only at the datapoints that are closest to this line; the SUPPORT VECTORS
#They are the datapoints that matter most.
#We try to find the hyperplane/line by using a specific equation that maximizes the margin between the support vectors and the line

#We can also use Soft Margin SVM that allows for some support vectors to be misclassified




